---
title: "Project #2"
author: "Winter Nguyen"
date: "`r Sys.Date()`"
output: pdf_document
urlcolor: blue
---
<!-- The author of this template is Dr. Gordan Zitkovic.-->
<!-- The code chunk below contains some settings that will  -->
<!-- make your R code look better in the output pdf file.  -->
<!-- If you are curious about what each option below does, -->
<!-- go to https://yihui.org/knitr/options/ -->
<!-- If not, feel free to disregard everything ...  -->
```{r echo=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.align="center",
  fig.pos="t",
  strip.white = TRUE
)
```
<!-- ... down to here. -->

---

## Problem #1 **(55 points)**
The `iris` data set is built-in in `R`. Start by studying the documentation of the data set, i.e., by entering `?iris` in the console. To familiarize yourselves with the architecture of an iris flower, go to: 

[US Forest Service](https://www.fs.usda.gov/wildflowers/beauty/iris/flower.shtml)

Your next step is exploratory data analysis. 

**(10 points)**
Which plot would you use to display pairwise associations between different measurements? How do you make sure that the different species are color-coded? Display the plot and write a few sentences about your conclusions. 
```{r}
species=iris$Species
species_colors = c("red", "purple", "cyan")[unclass(iris$Species)]
pairs(
  iris[, 1:4],
  main = "Iris Pairwise Scatterplots",
  labels =c("Sepal Length (cm)","Sepal Width (cm)", "Petal Length (cm)","Petal Width (cm)"),
  pch = 21,
  bg  = c("red", "purple", "cyan")[unclass(iris$Species)]
)

legend(
  "topleft",
  legend = levels(iris$Species),
  fill = c("red", "purple", "cyan"),
  cex = 0.8,
  xpd=TRUE
)

```
  I used the pairs() function to visualize the pairwise relationships among the measurements. From the plots, Petal Length and Petal Width show a strong positive linear relationship across all three species

  Sepal Length and Sepal Width exhibit a moderate positive correlation for Virginica and Versicolor, and this relationship is slightly stronger in Setosa. 

  Additionally, Sepal Length and Petal Length display a strong positive linear relationship for Versicolor and Virginica, but this relationship is not evident in Setosa. Similar patterns are observed for Sepal Length vs. Petal Width and Sepal Width vs. Petal Length, where Setosa differs from the other two species.

  Overall, there is a positive linear relationship among all the measurements, with petal dimensions showing the strongest correlations for Versicolor and Virginica. The variables behave differently in Setosa, where the relationships are generally weaker or follow a distinct pattern, but still strong between Pedal Length and Petal Width.

### Principal Component Analysis (PCA)
**($20$ points)**
Perform the PCA on the explanatory components of the above data, provide the report, and the relevant plots. 
```{r}
pr.out=prcomp(iris[, 1:4], scale=TRUE) # exclude Species because it is category

#I just change the rotation based on my preference
pr.out$rotation = -pr.out$rotation 
pr.out$x = -pr.out$x

#I changed the columns name, my preference
rownames(pr.out$rotation) =c("Sepal Length","Sepal Width", "Petal Length","Petal Width")
biplot(pr.out, scale = 0,
       cex=0.7,
      bg  = c("red", "purple", "cyan")[unclass(iris$Species)])
summary(pr.out)
```
Below I provide another PCA version to see more information.
```{r}
loadings=pr.out$rotation
scores=pr.out$x
plot(scores[,1], scores[,2], #PC1 and PC2 
     col=c("red", "purple", "cyan")[unclass(iris$Species)],
     pch=19,
     xlab="PC1",
     ylab="PC2",
     main="PCA with colored specices")

# adding vector for each variables
arrows(0, 0, loadings[,1]*2.5, loadings[,2]*2.5, length = 0.1, col = "black")
#labeling vectors
text(loadings[1,1]*2.7, loadings[1,2]*3.0, labels = rownames(loadings)[1], col = "black")
text(loadings[2,1]*2.7,loadings[2,2]*2.7,labels=rownames(loadings)[2],col="black")
text(loadings[3,1]*3.5,loadings[3,2]*2.7,labels=rownames(loadings)[3],col="black")
text(loadings[4,1]*3.2,loadings[4,2]*6,labels=rownames(loadings)[4],col="black")


```
  From the biplot, PC1 (horizontal) primarily captures overall flower size.Petal Length, and Petal Width all point strongly in the same (leftward) direction, Sepal Length also point to left but not as strong as Pedal variables, indicating that these three measurements are positively correlated and together represent the main source of variation among samples. PC2 (vertical) is dominated by Sepal Width, which points in the opposite (rightward) direction, suggesting that flowers with larger petals and longer sepals tend to have narrower sepals. The opposing directions of Sepal Width and the other three features indicate a negative correlation between sepal width and the overall flower size traits.
  
   Notice that, Setosa's cluster is distinct from the other two species, suggesting the size gap, Setosa is significantly smaller than the other two species.

```{r, fig.width=9, fig.height=8}
# I calculate the Proportion of Variance Explained by Principal Component.
pr.var = pr.out$sdev^2
pve=pr.var/sum(pr.var)

#plotting the proportion of variance explained vs principal component
plot(pve, type = "b",
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     main = "Scree Plot")

text(x = 1:length(pve),
     y = pve,
     labels = round(pve, 3),
     pos = 3,   # position: 3 = above
     cex = 0.8, # text size
     col = "blue")
```
  The plot confirms the idea that first component explain the most variance. The proportion of variance explained (PVE) by each principal component is approximately 72.96%, 22.85%, 3.67%, and 0.52% for PC1 through PC4, respectively, are consistent with the summary table. Together, the first two principal components explain about 95.8% of the total variance, indicating that most of the information in the data set can be effectively represented in two dimensions.

### Principal Components Regression (PCR)
Your next task is to predict `Sepal.Length` from the other variables in the `iris` dataset. 

**(15 points)**
Run the PCR, provide an explanation for the output, and display the relevant plots (both validation and prediction). 
```{r}
library(pls)
set.seed(1)
pcr.out= pcr(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,
                 data = iris,
                 scale = TRUE,   # standardize variables
                 validation = "CV")

#plot the validation
validationplot(pcr.out, val.type = "MSEP", main = "PCR Cross-validation")

#plot the predictions
predictions= predict(pcr.out, ncomp = 2, newdata = iris)
plot(iris$Sepal.Length, predictions,
     xlab = "Actual Sepal Length",
     ylab = "Predicted Sepal Length (PCR)",
     main = "PCR Prediction Plot",
     pch = 21, bg = "lightblue")
abline(0, 1, col = "red", lwd = 2)
summary(pcr.out)

```

  The validation plot tells us that there is a major drop in MSE at the first component (from 0.7 to 0.3, so 0.4 in value), this means the PCA1 captures major variation in predictors. Then it continues to drop 0.15 in value at PC2, and then continues to drop at PCA3 but not too much (0.05 in value). So this makes sense since according to the PCR summary, the first two principal components explained 98.84% of the variance. However,
This model using the same data to train and to test the model, meaning that the model has already seen those exact data points during training, so the predictions will be unrealistically good. This introduces bias and does not reflect real-world predictive performance.

**(10 points)**
Split your dataset into training ($4/5$ of the data) and testing ($1/5$ of the data). Provide the mean squared error and an appropriate plot.
```{r}
set.seed(200)

#spliting the data
training_index=sample(1:nrow(iris), size=(4/5)*nrow(iris))
training_set=iris[training_index,]
test_set=iris[-training_index,]

#performing pcr on splitting data
pcr_out_training=pcr(Sepal.Length~Sepal.Width + Petal.Length + Petal.Width, data=training_set, scale=TRUE, validation="CV")
validationplot(pcr_out_training, 
               val.type = "MSEP", 
               main = "PCR Cross-validation \n(4/5 Training Set, 1/5 Test Set)")

#ploting side by side the validation plot of full data set and splitted data set
par(mfrow = c(1, 2)) 
validationplot(pcr.out, 
               val.type = "MSEP", 
               main = "PCR Cross-validation")
validationplot(pcr_out_training, 
               val.type = "MSEP", 
               main = "PCR Cross-validation \n(4/5 Training Set, 1/5 Test Set)")
par(mfrow = c(1, 1))


```
  The full dataset PCR model and the model trained on the 4/5 training subset have nearly identical cross-validation MSEP curves. This indicates that the optimal number of components and the model’s predictive performance (as reflected by the MSEP) are stable across different data splits. The fact that the smaller 4/5 training set achieves a similar minimum MSEP to the full dataset suggests that the training set is sufficiently large and that adding more data would not substantially improve model accuracy. Since in both models, the MSEP only decreases (no increase after certain component) showing that there is no evidence of overfitting.
```{r}
#Using model on training set to predict sepal length on test set
predictions_on_test_set= predict(pcr_out_training, ncomp = 2, newdata =test_set )
mse = mean((test_set$Sepal.Length - predictions_on_test_set)^2)
bias= mean(test_set$Sepal.Length - predictions_on_test_set) # very close to 0 so 
                                                    # I assume it is unbias
range_sepal_length=range(iris$Sepal.Length)
```
  The MSE is `r round(mse,3)`. I calculated the bias which is the expected value of residuals and I got `r bias`, very close to 0, so I assume the model is unbias. Then the standard deviation calculated from MSE is `r round (sqrt(mse),3)`  that means on average, my predicted sepal lengths are off by `r round(sqrt(mse), digit=3)` cm which is small compared to the range of sepal lengths (`r range_sepal_length`) cm
```{r}
#plotting side by side the preidiction plot and also the residuals plot,
#making it easier to interpret.
par(mfrow = c(1, 2)) 
residuals_pred_real=test_set$Sepal.Length-predictions_on_test_set
plot(predictions_on_test_set,residuals_pred_real,
     xlab = "Predicted Sepal Length (PCR)",
     ylab = "Residuals",
     main = "Differences Between The Actual \nand Predicted Sepal Length",
     pch = 21, bg = "lightblue",
     ylim = c(-max(test_set$Sepal.Length),max(test_set$Sepal.Length))) # I change the
                            # range to illustrate how big or small residuals really is
                            # compared to its possible range
abline(h=0, col="red", lwd=2)

plot(test_set$Sepal.Length, predictions_on_test_set,
     xlab = "Actual Sepal Length",
     ylab = "Predicted Sepal Length (PCR)",
     main = "PCR Prediction Plot \n(4/5 Training Set, 1/5 Test Set)",
     pch = 21, bg = "lightblue")
abline(0, 1, col = "red", lwd = 2)
par(mfrow = c(1, 1))
```
  There is a strong linear relationship between predicted and actual sepal lengths which is what we should expect. In the residual plot, we can see the residuals scatter around the zero line, and relatively small compared to the range of possible values of residual. 

```{r}
#this part is to check on my interpretation more accurately
lm_pred_real=lm(predictions_on_test_set~test_set$Sepal.Length)
summary(lm_pred_real)
```
  With p-value is less than 0.05, and the coefficient correlation R-squared = `r 0.78` which is a strong but not perfect correlation, confirming my previous interpretation. Also, the residual standard error is `r 0.3612` which is consistent with my RSE is `r round(sqrt(mse), digit=3)` (calculated RSE from MSE). The small difference maybe due to degree of freedom adjustment. 

## Problem #2 **(20+5+10+10=45 points)**
Solve **Problem 3.7.15** (page 128) from the textbook. 

*Hint:* The command `lapply` could be useful. 

This problem involves the Boston data set, which we saw in the lab
for this chapter. We will now try to predict per capita crime rate
using the other variables in this data set. In other words, per capita
crime rate is the response, and the other variables are the predictors.

CRIM - per capita crime rate by town
ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
INDUS - proportion of non-retail business acres per town.
CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
NOX - nitric oxides concentration (parts per 10 million)
RM - average number of rooms per dwelling
AGE - proportion of owner-occupied units built prior to 1940
DIS - weighted distances to five Boston employment centres
RAD - index of accessibility to radial highways
TAX - full-value property-tax rate per \$10,000
PTRATIO - pupil-teacher ratio by town
BLACK - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
LSTAT - % lower status of the population
MEDV - Median value of owner-occupied homes in \$1000's

(a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions
```{r}
# we predict per capital crime rate
# so I extract all other predictors except "crim"
library(MASS)
library(car)
predictors=Boston[,2:length(Boston)] # "crim" index is 1, so I exclude it
crim_predictors= lapply(predictors, function(x) lm(Boston$crim ~ x)) # perform simple                                                          #linear model for each predictor
results = data.frame(
  Predictor = names(predictors),
  Coefficient = sapply(crim_predictors, function(y) coef(y)[2]),
  P_value = sapply(crim_predictors, function(y) summary(y)$coefficients[2, 4])
)
results
```
  Based on the results, all the p-values except for "chas" are under 0.05. So except "chas", there are significant associations between those predictors and "crim". 

```{r}
#performing linear regression on all predictors except "chas"
# Make sure predictors are correctly defined by name
chas = predictors$chas
dis  = predictors$dis
rad  = predictors$rad

# Fit models
crim_vs_chas = lm(Boston$crim ~ chas)
crim_vs_dis  = lm(Boston$crim ~ dis)
crim_vs_rad  = lm(Boston$crim ~ rad)


pairs(Boston[, c("crim", "dis", "rad", "chas")],
      main = "Pairwise Relationships with crim",
      col = "darkorange", pch = 19)
```
From the pairwise relationships, "dis" shows a strong negative nonlinear association with crime rate, suggesting that areas farther from employment centers tend to have lower crime. In contrast, rad has a positive association: neighborhoods with higher highway accessibility exhibit higher crime levels. The chas variable, representing proximity to the Charles River, shows no clear pattern, consistent with its lack of statistical significance. However, these plots are influenced by outliers and variable types: crim is heavily right-skewed, "rad" and "chas" are discrete, and several relationships are nonlinear. So, it is better to use polynomial models and using boxplots for categorical predictors, or we can transform crim to high and low crim and apply logistic regression.
```{r}
summary(crim_vs_chas)
```
The relationship between crime rate and proximity to the Charles River "chas" is weak and not statistically significant (p = 0.209). Neighborhoods near the river tend to have slightly lower crime on average, but the difference is small and not meaningful. Since there is no relationship, I will skip the boxplot.
```{r}
summary(crim_vs_dis)
```
There is a strong and statistically significant negative relationship between crime rate and distance to employment centers "dis". Areas farther from the city (larger "dis") tend to have substantially lower crime rates.
```{r}
summary(crim_vs_rad)
```
The accessibility to radial highways "rad" has a strong positive and statistically significant association with crime rate. Neighborhoods with greater access to highways tend to experience much higher crime levels, and rad alone explains nearly 40% of the variability in crime. Below is the box plot for each area.

```{r}
boxplot(crim ~ as.factor(rad), 
        data = Boston,
        main = "Crime Rate by Highway Accessibility (rad)",
        xlab = "Accessibility to Radial Highways (rad index)",
        ylab = "Per Capita Crime Rate (crim)",
        col = "lightblue",
        border = "gray40",
        cex.axis = 0.8)
```

(b) Fit a multiple regression model to predict the response using
all of the predictors. Describe your results. For which predictors
can we reject the null hypothesis $H_0: \beta_j = 0$

```{r}
# perform multiple linear regression with reponse to be criminal rate per capita
# the rest are predictors
mlr=lm(crim~., data=Boston)
summary(mlr)
```
  Overall, the model demonstrates a moderate fit, explaining about 45% of the variation in crime rates among Boston neighborhoods, the p value is less than 0.05, so there are significant association between response and predictors ,however, this doesn't apply for each predictor. Based on the summary, there are associations between predictors and "crim" only the following predictors: "zn" , "dis", "rad", "black", and "medv" (we reject the null hypothesis for these predictors).

(c) How do your results from (a) compare to your results from (b)?
Create a plot displaying the univariate regression coefficients
from (a) on the x-axis, and the multiple regression coefficients
from (b) on the y-axis. That is, each predictor is displayed as a
single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate
in the multiple linear regression model is shown on the y-axis.

  In b) we eliminate most of the predictors ("indus", "chas", "nox", "rm", "age", "tax", "ptratio", "lstat") compared to just "chas" in a). This may result from correlation between predictors - multicollinearity. So, only "zn", "dis", "rad", "black" have significant associations with "crim".
```{r}
multi_coef= coef(mlr)[-1]  # remove intercept
multi_results= data.frame(
  Predictor= names(multi_coef),
  Multiple_Coefficient = multi_coef)
comparison= merge(results, multi_results, by = "Predictor")
head(comparison)

#plotting
plot(comparison$Coefficient, comparison$Multiple_Coefficient,
     xlab = "Simple Regression Coefficient (a)",
     ylab = "Multiple Regression Coefficient (b)",
     main = "Comparison of Coefficients:\n Simple vs Multiple Regression",
     pch = 21, bg = "lightblue")
abline(0, 1, col = "red", lwd = 2)  # line of equality
text(comparison$Coefficient, comparison$Multiple_Coefficient,
     labels = comparison$Predictor, pos = 4, cex = 0.7)
vif(mlr)
```
  The comparison plot between simple and multiple regression coefficients shows that coefficients change substantially in magnitude and even direction (namely "nox"). This suggests strong multicollinearity in the Boston dataset. Some predictors, like nox and indus, are highly correlated with others, leading to inflated coefficients in the simple models. Predictors close to the diagonal line, "rad" for example, maintain similar effects across both models, implying independent predictors. 
  
  To assess multicollinearity among predictors, the Variance Inflation Factor (VIF) was computed for all variables in the multiple linear regression model. Most predictors had VIF values below 5, indicating acceptable levels of correlation. However, two variables  "rad" (VIF = 7.16) and "tax" (VIF = 9.20) exhibited high multicollinearity. This suggests that a large portion of their variance can be explained by other predictors, meaning they provide overlapping information rather than unique contributions to the model.

Both rad and tax describe aspects of urban infrastructure:

rad measures accessibility to radial highways, and

tax reflects property tax rates that are typically higher in densely urbanized areas.

Because these two variables are highly correlated with each other (correlation approximately 0.9), including both can inflate standard errors and make coefficient estimates unstable.

To address this issue, "tax" should be from the model since it is less interpretable in the context of crime prediction, while "rad" was retained as it provides a clearer physical interpretation of accessibility and urbanization.

```{r}
new_data=Boston[, -10] #tax is 10, eliminate tax
new_mlr= lm(crim~., data=new_data)
vif(new_mlr)
```
After removing tax, all remaining predictors had VIF values below 5, indicating that multicollinearity was no longer a concern and the model’s estimates became more stable and reliable.

(d) Is there evidence of non-linear association between any of the
predictors and the response? To answer this question, for each
predictor X, fit a model of the form
$$
Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon
$$
```{r}
# create a data frame for nonlinear-model,
# containing predictors name, its quadratic p-value and cubic p-value
nonlinear_results = data.frame(Predictor = names(predictors),
                                X2 = NA, 
                                X3 = NA)
for (i in 1:length(predictors)) {
  x = predictors[[i]]
  model = lm(Boston$crim ~ x + I(x^2) + I(x^3))
  coefs = summary(model)$coefficients
  if (nrow(coefs) >= 4) {  # check that cubic term exists
    nonlinear_results$X2[i] = coefs[3, 4] #[3,4] p-value for quadratic term
    nonlinear_results$X3[i] = coefs[4, 4] #[4,4] p-value for cubic term
  }
}
nonlinear_results
```
  Based on the results, many predictors have non-linear associations. For adding the cubic terms, the following predictors : "indus", "nox", "age", "dis", ptratio, "medv" became significant. When only quadratic terms were added, the predictors: "zn", "rm", "rad", "tax", "lstat" were significant but lost significance after including the cubic terms. Only "black" has no non-linear relationship, as adding quadratic or cubic terms did not make its association significant.
  
```{r}
nox = predictors$nox
black  = predictors$black
medv  = predictors$medv

# Fit models
crim_vs_black = lm(Boston$crim ~ black)
crim_vs_nox  = lm(Boston$crim ~ nox)
crim_vs_medv  = lm(Boston$crim ~ rad)


pairs(Boston[, c("crim", "nox", "medv", "black")],
      main = "Pairwise Relationships with crim",
      col = "darkorange", pch = 19)
```
The predictors "nox", "medv", and "black" were chosen to represent environmental, economic, and demographic factors related to crime, also they have different relationship with "crim". The plot shows that "nox" has a non-linear positive relationship with crime, indicating higher crime in more urbanized areas. "medv" is non-linear strongly negatively related to crime, suggesting wealthier neighborhoods experience less crime. The relationship between "black" and crime appears weaker and less consistent (a U-shape), possibly reflecting more complex social factors. 